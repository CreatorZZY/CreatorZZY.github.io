---
layout: post
title: 熵(Entropy)
subtitle: Note.
#image: /img/2019-08-04_23.18.15.jpg
tags: [Note]
bigimg: /img/2020-03-06-熵(Entropy)/2020-03-06-熵(Entropy).jpg
gh-repo: CreatorZZY
gh-badge: [follow]
comments: true
---

熵(Entropy)
===
## 信息熵
---
考虑一个离散的随机变量 x，由上面两个例子可知，信息的量度应该依赖于概率分布 p(x)，因此我们想要寻找一个函数 I(x)，它是概率 p(x) 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 x 和 y，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：
$$I(x, y)=I(x)+I(y)$$
这么一看，就知道Log要上场了。
$$log_a(mn)=log_am+log_an$$
而且需要符号保证
$$I(x)=-\log p(x)$$
**平均信息量**
注意一下：联合熵：
$$H(X, Y)=-\sum_{x, y} p(x, y) \log p(x, y)=-\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i}, y_{i}\right) \log p\left(x_{i}, y_{i}\right)$$
非均匀分布比均匀分布的熵要小。现在让我们考虑如何把变量状态的类别传递给接收者。与之前一样，我们可以使用一个2比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件。有点像数据结构讲ZIP压缩算法提到的那个**霍夫曼树**的那个玩意儿。
> 熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）

## 条件熵
---
条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。**条件熵 H(Y|X) 定义为 X 给定条件下 Y 的条件概率分布的熵对  X 的数学期望**：
$$
\begin{aligned} H(Y | X) &=\sum_{x} p(x) H(Y | X=x) \\ &=-\sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x) \\ &=-\sum_{x} \sum_{y} p(x, y) \log p(y | x) \\ &=-\sum_{x, y} p(x, y) \log p(y | x) \end{aligned}
$$

$$
\begin{aligned} H(X, Y) &=-\sum_{x, y} p(x, y) \log p(x, y) \\ &=-\sum_{x, y} p(x, y) \log (p(y | x) p(x)) \\ &=-\sum_{x, y} p(x, y) \log p(y | x)-\sum_{x, y} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x, y} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x} \sum_{y} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x} \log p(x) \sum_{y} p(x, y) \\ &=H(Y | X)-\sum_{x}(\log p(x)) p(x) \\ &=H(Y | X)-\sum_{x} p(x) \log p(x) \\ &=H(Y | X)+H(X) \end{aligned}
$$

当知道了X时，X,Y联合发生的信息量减小
$$
H(Y | X)=H(X, Y)-H(X)
$$

## 相对熵
---
设 p(x)、q(x) 是 离散随机变量 X 中取值的两个**概率分布**，则 p 对 q 的相对熵是：
$$
D_{KL}(p||q)=\displaystyle\sum_{x}p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
$$
其中其他的都很好理解，就是这个：$D_{K L}(p \| q) \geq 0$，利用利用Jensen不等式证明：
$$\begin{aligned}
D_{K L}(p \| q) &=\sum_{x} p(x) \log \frac{p(x)}{q(x)} \\
&=-\sum_{x} p(x) \log \frac{q(x)}{p(x)} \\
&=-E_{p(x)}\left(\log \frac{q(x)}{p(x)}\right) \\
& \geq-\log E_{p(x)}\left(\frac{q(x)}{p(x)}\right) \\
&=-\log \sum_{x} p(x) \frac{q(x)}{p(x)} \\
&=-\log \sum_{x} q(x)
\end{aligned}$$
通过放缩，其实很好理解，一个小于1的正数本身就是在拉低整个函数的值，这里只需要考虑$\frac{p(x)}{q(x)}<0$的情况就行，因为这个情况**最危险**

## 交叉熵
---
如果用真实分布 p(x) 来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:
$$H(p)=\sum_{x} p(x) \log \frac{1}{p(x)}$$
如果使用非真实分布 q(x) 来表示来自真实分布 p(x) 的平均编码长度，则是：
$$H(p, q)=\sum_{x} p(x) \log \frac{1}{q(x)}$$
值得注意的是：（当用非真实分布 q(x) 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵）
$$D_{K L}(p \| q)=\sum_{x} p(x) \log \frac{p(x)}{q(x)}=\sum_{x} p(x) \log p(x)-p(x) \log q(x)$$
$$D_{K L}(p \| q)=H(p, q)-H(p)$$

## 参考
[详解机器学习中的熵、条件熵、相对熵和交叉熵 - 遍地胡说 - 博客园](https://www.cnblogs.com/kyrieng/p/8694705.html)